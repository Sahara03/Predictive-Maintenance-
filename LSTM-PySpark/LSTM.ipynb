{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HADOOP_USER_NAME\"]=\"hdfs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telemetry=sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true', inferschema='true').load('hdfs://172.17.0.3:9000/user/root/data/PdM_telemetry.csv')\n",
    "errors=sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true', inferschema='true').load('hdfs://172.17.0.3:9000/user/root/data/PdM_errors.csv')\n",
    "maint=sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true', inferschema='true').load('hdfs://172.17.0.3:9000/user/root/data/PdM_maint.csv')\n",
    "failures=sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true', inferschema='true').load('hdfs://172.17.0.3:9000/user/root/data/PdM_failures.csv')\n",
    "machines=sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true', inferschema='true').load('hdfs://172.17.0.3:9000/user/root/data/PdM_machines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "\n",
    "import lstmOps\n",
    "import utilities_classifiersOps as classifiersOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('dataset_24h.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, labelcolumns, trainScaler = classifiersOps.preprocessing_Multiclass(\n",
    "    df, labelName=['failure'], dismissCols=['datetime', 'machineID'], dummies=True, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "spliter=StratifiedShuffleSplit(test_size=0.33)\n",
    "train_indexes = []\n",
    "test_indexes= []\n",
    "\n",
    "for train_index, test_index in spliter.split(features, np.argmax(labels, axis=1)):\n",
    "    train_indexes.append(train_index)\n",
    "    test_indexes.append(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = features[train_indexes[-1]], labels[train_indexes[-1]]\n",
    "x_test, y_test = features[test_indexes[-1]], labels[test_indexes[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_feat, batched_label = lstmOps.split_sequences(x_train, y_train, 24)\n",
    "batchedX_test, batchedY_test = lstmOps.split_sequences(x_test, y_test, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons=50\n",
    "n_epoch=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=batchedX_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(stateful):\n",
    "    model=Sequential()\n",
    "    model.add(LSTM(50,\n",
    "                   activation='relu',\n",
    "                  input_shape=(24,31)))\n",
    "    model.add(Dense(5))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(stateful):\n",
    "    model=Sequential()\n",
    "    model.add(LSTM(50,\n",
    "                   activation='relu',\n",
    "                  input_shape=(24,31)))\n",
    "    model.add(Dense(5))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stateful = create_model(stateful=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model_stateful.fit(batched_feat,batched_label,\n",
    "                                          epochs=5,\n",
    "                     batch_size=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=model_stateful.predict(batchedX_test, batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invertScale(batchs_array, scaler):\n",
    "    if len(batchs_array.shape) == 3:\n",
    "        invert = []\n",
    "        for i in range(batchs_array.shape[1]):\n",
    "            inverted_batch = scaler.inverse_transform(batchs_array[:,i,:])\n",
    "            inverted.appends(inverted_batch)\n",
    "        return np.array(inverted).reshape(batchs_array.shape[0], batchs_array.shape[1], batchs_array.shape[2])\n",
    "    else: \n",
    "        return scaler.inverse_transform(batchs_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, features, scaler):\n",
    "    preds=[]\n",
    "    for t in range(features.shape[0]):\n",
    "        pred = invertScale(model.predict(features[t].reshape(1,features[-1].shape[0],features[-1].shape[1])), scaler)\n",
    "        preds.append(pred)\n",
    "    preds = np.array(preds)\n",
    "    preds = preds.reshape(preds.shape[0],preds.shape[2])\n",
    "    return preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
